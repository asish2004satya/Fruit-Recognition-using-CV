{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10300 images successfully!\n"
     ]
    }
   ],
   "source": [
    "data_path = '/Users/asish/Documents/Projects/Internship project/fruits-360/Training'  \n",
    "fruit_names = []\n",
    "fruit_images = []\n",
    "max_images_per_class = 50  # Start with only 50 pictures per fruit to save memory\n",
    "\n",
    "# Load pictures\n",
    "for folder in os.listdir(data_path):\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        count = 0\n",
    "        for file in os.listdir(folder_path):\n",
    "            if count >= max_images_per_class:\n",
    "                break\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    print(f\"Could not read image: {img_path}\")\n",
    "                    continue\n",
    "                img = cv2.resize(img, (224, 224))\n",
    "                fruit_images.append(img)\n",
    "                fruit_names.append(folder)\n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "# Check if images were loaded\n",
    "if len(fruit_images) == 0:\n",
    "    print(\"No images loaded! Check your data_path or dataset.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"Loaded {len(fruit_images)} images successfully!\")\n",
    "\n",
    "# Turn pictures and names into numbers\n",
    "fruit_images = np.array(fruit_images) / 255.0\n",
    "label_encoder = LabelEncoder()\n",
    "fruit_labels = label_encoder.fit_transform(fruit_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(fruit_images, fruit_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 330ms/step - accuracy: 0.2133 - loss: 4.0808 - val_accuracy: 0.7403 - val_loss: 1.1467\n",
      "Epoch 2/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 320ms/step - accuracy: 0.8503 - loss: 0.7644 - val_accuracy: 0.8917 - val_loss: 0.4547\n",
      "Epoch 3/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 354ms/step - accuracy: 0.9413 - loss: 0.3053 - val_accuracy: 0.9432 - val_loss: 0.2510\n",
      "Epoch 4/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 379ms/step - accuracy: 0.9713 - loss: 0.1595 - val_accuracy: 0.9665 - val_loss: 0.1650\n",
      "Epoch 5/5\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 357ms/step - accuracy: 0.9822 - loss: 0.1001 - val_accuracy: 0.9689 - val_loss: 0.1319\n"
     ]
    }
   ],
   "source": [
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 236ms/step - accuracy: 0.9730 - loss: 0.1220\n",
      "The computer got 96.89% correct!\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"The computer got {test_accuracy * 100:.2f}% correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 220ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 8,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0, 11,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0, 10, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  7,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  7,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0, 10]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "The computer thinks this is a Pear Red 1!\n",
      "Error loading /Users/asish/Documents/Projects/Internship project/newImage.jpg: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::(anonymous namespace)::CvtHelper<cv::impl::(anonymous namespace)::Set<3, 4>, cv::impl::(anonymous namespace)::Set<3, 4>, cv::impl::(anonymous namespace)::Set<0, 2, 5>>::CvtHelper(InputArray, OutputArray, int) [VScn = cv::impl::(anonymous namespace)::Set<3, 4>, VDcn = cv::impl::(anonymous namespace)::Set<3, 4>, VDepth = cv::impl::(anonymous namespace)::Set<0, 2, 5>, sizePolicy = cv::impl::(anonymous namespace)::NONE]'\n",
      "> Unsupported depth of input image:\n",
      ">     'VDepth::contains(depth)'\n",
      "> where\n",
      ">     'depth' is 6 (CV_64F)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_img_path = '/Users/asish/Documents/Projects/Internship project/newImage.jpg'  # CHANGE to your picture path\n",
    "try:\n",
    "    new_img = cv2.imread(new_img_path)\n",
    "    if new_img is None:\n",
    "        print(f\"Could not read image: {new_img_path}\")\n",
    "    else:\n",
    "        new_img = cv2.resize(new_img, (224, 224))\n",
    "        new_img = np.array(new_img) / 255.0\n",
    "        new_img = np.expand_dims(new_img, axis=0)\n",
    "        prediction = model.predict(new_img)\n",
    "        predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "        predicted_fruit = label_encoder.classes_[predicted_class]\n",
    "        print(f\"The computer thinks this is a {predicted_fruit}!\")\n",
    "        plt.imshow(cv2.cvtColor(new_img[0], cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f'Predicted: {predicted_fruit}')\n",
    "        plt.savefig('predicted_apple.jpg')\n",
    "        plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {new_img_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
